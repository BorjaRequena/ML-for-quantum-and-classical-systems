{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "22-01-25_Interpretable_ML_CAM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNDQCDSFVdQ4"
      },
      "source": [
        "# **Interpretability methods for neural networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seqEdVR1VdQ7"
      },
      "source": [
        "## Introduction\n",
        "The general rule is, the more complex model, the more difficult problems it can solve (well, makes sense, right?). But also the more complex model, the less interpretable it becomes.\n",
        "\n",
        "### Why do we bother with interpretability at all?\n",
        "\n",
        "* making the model more reliable -> when we analyze what features are the most important for the model during classification, and we find out the model looks at something irrelevant to the human, we can fight it and make the model more reliable. E.g.: **[picture](http://3.bp.blogspot.com/-S8g8SNWhyR0/WIsOFqPD2WI/AAAAAAAAA68/9yNFp6sdao0Er7qDIqEPu7ORTU589tFCACK4B/s1600/Bildschirmfoto%2B2017-01-27%2Bum%2B10.08.11.png): husky on the snow => prediction: wolf**\n",
        "\n",
        "\n",
        "* ethical reasons: preventing biases inherited from humans and increasing trust in models applied in real-life problems, e.g., helping in medial diagnosis\n",
        "* legal reasons: since May 2019 every EU citizen has the right to know the reasoning process behind every automated decision that concerns him/her\n",
        "* the only way for ML to teach us something new about physics?\n",
        "\n",
        "There are ML models that are inherently interpretable (up to some complexity degree), like decision trees or linear models.\n",
        "\n",
        "The most common example of a complex uninterpretable ML model are deep neural networks (including convolutional neural networks, being especially useful in image classification)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFpUrBiRgFiV"
      },
      "source": [
        "## What methods are there?\n",
        "\n",
        "Their number is huge and growing fast. Some of the interpretability methods can be found in packages like [iNNvestigate](https://github.com/albermax/innvestigate/blob/master/examples/notebooks/mnist_compare_methods.ipynb), [InterpretML](https://arxiv.org/abs/1909.09223) or [Alibi](https://github.com/SeldonIO/alibi).\n",
        "\n",
        "\n",
        "## What will we do today?\n",
        "\n",
        "We will implement the Class Activation Maps (CAM) [(original paper)](https://arxiv.org/pdf/1512.04150.pdf). They have many extensions (e.g., [Grad-CAM](https://jacobgil.github.io/deeplearning/class-activation-maps))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K42wAvdfOML"
      },
      "source": [
        "# Class Activation Maps (CAM)\n",
        "\n",
        "Code based on [the code provided by the method's authors](https://github.com/zhoubolei/CAM)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEkPKbxiWWu-"
      },
      "source": [
        "![CAM](https://drive.google.com/uc?id=1vibsld9iq7j4GYgDP5LMXdoitvfr8q_M)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ows63OwwMZ8i"
      },
      "source": [
        "## We'll start from importing a ready-to-use neural network **with GAP in the end**\n",
        "\n",
        "We will use CAMs to explain predictions of ResNet on images from the dataset ImageNet. It is a collection of over 14 millions photos belonging to [1000](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a) categories (to be exact, 1000 non-overlapping categories, in general 20 000 classes are listed there, but NNs race on the dataset with separable classes).\n",
        "\n",
        "Designing and training from scratch an accurate NN recognizing such a number of objects would be tough and time-consuming so let's stand on the arms of the giants, and import a NN that is ready-to-use! :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpfU-WXKg5CY"
      },
      "source": [
        "import io\n",
        "import requests\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow # for image display\n",
        "\n",
        "from torchvision import models, transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pdb"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZq-GfWZK5-Z"
      },
      "source": [
        "# Let's decide on the model first. Maybe print it out so we see the architecture.\n",
        "# Remember, for the basic CAM, the global average pooling (GAP) is needed! Every feature map has to have only one value in the fully-connected layer!\n",
        "# networks such as GoogleNet, ResNet, DenseNet already use global average pooling at the end, so CAM could be used directly.\n",
        "# You can look at them here: https://neurohive.io/en/popular-networks/\n",
        "model_id = 2 # 1 = SqueezeNet, 2 = ResNet18, 3 = DenseNet161\n",
        "\n",
        "if model_id == 1:\n",
        "    net = models.squeezenet1_1(pretrained=True)\n",
        "    finalconv_name = 'features' # this is the last conv layer of the network\n",
        "elif model_id == 2:\n",
        "    net = models.resnet18(pretrained=True)\n",
        "    finalconv_name = 'layer4'\n",
        "elif model_id == 3:\n",
        "    net = models.densenet161(pretrained=True)\n",
        "    finalconv_name = 'features'\n",
        "\n",
        "#print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now let's take exemplary images.\n",
        "And let's make some predictions! To make predictions, we need to download the list of 1000 categories used by the pre-trained models. Why? Because when making predictions, the model will only tell us the number of the class. We need to find its name."
      ],
      "metadata": {
        "id": "Z1IOrPc3O6ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's start with loading all class names"
      ],
      "metadata": {
        "id": "W7N874h8QL6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the imagenet category list:\n",
        "LABELS_URL = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
        "classes = {int(key):value for (key, value) in requests.get(LABELS_URL).json().items()}"
      ],
      "metadata": {
        "id": "U_EMwbQpPP10"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print them. They don't look too nice, let's leave only names!\n",
        "print(classes)"
      ],
      "metadata": {
        "id": "kos3aTyuPn2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [classes[i][1] for i in range(len(classes))]"
      ],
      "metadata": {
        "id": "uqENKVrdPvc1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean classes for our use!\n",
        "classes[:10]"
      ],
      "metadata": {
        "id": "ikc7SQYUQGRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time for some images!"
      ],
      "metadata": {
        "id": "Bv9yeP9hQS2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of images to classify\n",
        "# you can play with it, just be careful with putting large images. Too large sizes will give you errors like \"I don't recognize this object\"\n",
        "images = [\n",
        "          ('cat', 'https://placekitten.com/800/571'),\n",
        "          ('dog', 'https://post.healthline.com/wp-content/uploads/2020/08/3180-Pug_green_grass-732x549-thumbnail-732x549.jpg'),\n",
        "          ('wine', 'https://www.thetimes.co.uk/imageserver/image/methode%2Fsundaytimes%2Fprod%2Fweb%2Fbin%2F5e7dc0d2-9b24-11e8-9837-8c5d4fba4ce3.jpg?crop=2667%2C1500%2C0%2C0&resize=685')\n",
        "         ]"
      ],
      "metadata": {
        "id": "d6bZy4DFQWGP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To classify an arbitrary image, we need to do some preprocessing to it before inputting to the model.\n",
        "# Namely: (1) Resize to (224, 224) so it fits the input size of our network.\n",
        "# (2) Transform the data into the format called \"Tensor\" which is used by PyTorch.\n",
        "# (3) Normalize the image accordingly to the mean and standard deviation of the training data in ImageNet dataset .\n",
        "\n",
        "normalize = transforms.Normalize(\n",
        "   mean=[0.485, 0.456, 0.406],\n",
        "   std=[0.229, 0.224, 0.225])\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "   transforms.Resize((224,224)),\n",
        "   transforms.ToTensor(),\n",
        "   normalize])"
      ],
      "metadata": {
        "id": "PdO-qLziSycJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model into evaluation mode\n",
        "net.eval()"
      ],
      "metadata": {
        "id": "0bS0H6G9RVqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,3,figsize=(15,5))\n",
        "fig.suptitle('True label / 1st, 2nd and 3rd best predictions', fontsize=16)\n",
        "\n",
        "for i, image in enumerate(images):\n",
        "    \n",
        "    # Get image\n",
        "    class_name, img_url = image[0], image[1]\n",
        "    response = requests.get(img_url)\n",
        "    img_pil = Image.open(io.BytesIO(response.content))\n",
        "\n",
        "    # We preprocess it\n",
        "    img_tensor = preprocess(img_pil)\n",
        "    img_variable = Variable(img_tensor.unsqueeze(0))\n",
        "\n",
        "    # Get predictions for all classes\n",
        "    preds = F.softmax(net(img_variable), dim=1).data.squeeze()\n",
        "\n",
        "    # Sort predictions\n",
        "    preds_sorted, idxs = preds.sort(descending=True)\n",
        "    \n",
        "    # Get best 3 predictions - classes\n",
        "    pred_1_class = classes[idxs[0]]\n",
        "    pred_2_class = classes[idxs[1]]\n",
        "    pred_3_class = classes[idxs[2]]\n",
        "    \n",
        "    # Get best 3 predictions - probabilities\n",
        "    pred_1_prob = np.round(100*preds_sorted[0].item(),2)\n",
        "    pred_2_prob = np.round(100*preds_sorted[1].item(),2)\n",
        "    pred_3_prob = np.round(100*preds_sorted[2].item(),2)\n",
        "    preds_best3 = [f'{pred_1_class} ({pred_1_prob}%)', f'{pred_2_class} ({pred_2_prob}%)', f'{pred_3_class} ({pred_3_prob}%)']\n",
        "    \n",
        "    # Display images with True label / 1st, 2nd and 3rd best Predictions\n",
        "    ax[i].imshow(img_pil)\n",
        "    ax[i].set_title(f'(true label) {class_name}\\n(1st) {preds_best3[0]}\\n(2nd) {preds_best3[1]}\\n(3rd) {preds_best3[2]}')"
      ],
      "metadata": {
        "id": "yVhh2edrQt0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's focus now on a single image and single prediction. How to interpret it?\n",
        "We know how to make predictions with ResNet which can classify objects on images into 1000 classes. Let's implement CAM and make ResNet tell us what it looks at, when thinking of each class."
      ],
      "metadata": {
        "id": "a6xj-MslU18j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_rTipdKigUa"
      },
      "source": [
        "# input image\n",
        "IMG_URL = 'https://www.oldtownmanor.com/wp-content/uploads/2015/01/Purebred-Breeders-LLC-Dog-Friendly-Airline.jpg'\n",
        "\n",
        "# print the input image\n",
        "response = requests.get(IMG_URL)\n",
        "img_pil = Image.open(io.BytesIO(response.content))\n",
        "plt.imshow(img_pil)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MiOF7PKbZIj"
      },
      "source": [
        "# If you want to save your results, you need to mount the Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "folder = '/content/drive/My Drive/Colab Notebooks/'\n",
        "\n",
        "#response = requests.get(IMG_URL)\n",
        "#img_pil = Image.open(io.BytesIO(response.content))\n",
        "img_pil.save(folder + 'test.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vnjg5ED6jaf6"
      },
      "source": [
        "# Hook the feature extractor. It means that now will be following what gets activated in the last convolutional layer.\n",
        "# So when we finally make a prediction on the test image with our model, this hook will contain the activated features.\n",
        "# The name of the last convolutional layer depend on the model we chose. In the cell where we did that, we also defined 'finalconv_name'.\n",
        "features_blobs = []\n",
        "def hook_feature(module, input, output):\n",
        "    features_blobs.append(output.data.cpu().numpy())\n",
        "\n",
        "net._modules.get(finalconv_name).register_forward_hook(hook_feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w1dS3-_lEj1"
      },
      "source": [
        "# Get the softmax weight (on the side of GAP, therefore 'params[-2]', so second from the end), so we take these single values to which every feature maps collapsed when we applied GAP.\n",
        "params = list(net.parameters())\n",
        "weight_softmax = np.squeeze(params[-2].data.numpy())"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuSRKK5ylFSO"
      },
      "source": [
        "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
        "    # generate the class activation maps upsample to 256x256\n",
        "    size_upsample = (256, 256)\n",
        "    bz, nc, h, w = feature_conv.shape\n",
        "    output_cam = []\n",
        "    for idx in class_idx:\n",
        "        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n",
        "        cam = cam.reshape(h, w)\n",
        "        cam = cam - np.min(cam)\n",
        "        cam_img = cam / np.max(cam)\n",
        "        cam_img = np.uint8(255 * cam_img)\n",
        "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
        "    return output_cam"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAwwUVqQh6a4"
      },
      "source": [
        "img_tensor = preprocess(img_pil)\n",
        "img_variable = Variable(img_tensor.unsqueeze(0))\n",
        "\n",
        "# Finally, the prediction! Hook attached to the final convolutional layer will remember the activated features.\n",
        "logit = net(img_variable)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lQ76YRqgOFH"
      },
      "source": [
        "# We put the logit through the softmax:\n",
        "h_x = F.softmax(logit, dim=1).data.squeeze()\n",
        "\n",
        "# We sort the probabilities along with the corresponding labels\n",
        "probs, idx = h_x.sort(0, True)\n",
        "probs = probs.numpy()\n",
        "idx = idx.numpy()\n",
        "\n",
        "# Output the prediction (so here 5 labels with the highest probabilities)\n",
        "for i in range(0, 10):\n",
        "    print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_CkyQlfGzvc"
      },
      "source": [
        "# Finally, we can generate class activation mapping for the any class prediction.\n",
        "# Let's start with the class with the highest probability, so top1: idx[0] (we sorted it accordingly to the probabilities, remmember?)\n",
        "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klnnqX1yG1tm"
      },
      "source": [
        "# Render the CAM and output\n",
        "print('output CAM.jpg for the top1 prediction: %s'%classes[idx[0]])\n",
        "img = cv2.imread(folder + 'test.jpg')\n",
        "print(img.shape)\n",
        "height, width, _ = img.shape\n",
        "heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
        "result = heatmap * 0.3 + img * 0.5\n",
        "cv2.imwrite(folder + 'CAM.jpg', result)\n",
        "cv2_imshow(result) # Colab and cv2.imshow() don't work, that's why we imported a patch with cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbK8g57PS-6N"
      },
      "source": [
        "# Tasks for today / Homework:\n",
        "\n",
        "## Numerical tasks:\n",
        "1. With \"copy-paste\", create a function which inputs an image URL + id of the model and outputs the CAMs of three classes that have the highest probabilities.\n",
        "\n",
        "2. Follow the shapes of: \n",
        "*   the input image\n",
        "*   the output of the model\n",
        "*   the size of the last layer (after GAP)\n",
        "*   what CAM function exactly does? What is 'weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))', why then the result is reshaped to (h, w)? What are 'bz, nc, h, w'? Print them out.\n",
        "*   what cv2.resize does? Why is it needed?\n",
        "\n",
        "## Discuss:\n",
        "1. Apply this method to a few images and discuss shortly what one can learn from CAMs and what are their limitations.\n",
        "2. Apply this method to a few images of the same category (e.g., various dogs, various cats) and again discuss shortly what one can learn from CAMs.\n",
        "3. Based on classifications described during the lecture, what categories CAM belongs to?\n",
        "  *   Model-specific or model-agnostic?\n",
        "  *   Provides local or global explanations?\n",
        "  *   Is it (a) a surrogate approach? (b) method focused on the model's component analysis? (c) method focused on analysis of the model after data perturbation?\n",
        "\n",
        "\n"
      ]
    }
  ]
}